def add(a, b):
    """
    Ð¡ÐºÐ»Ð°Ð´Ñ‹Ð²Ð°ÐµÑ‚ Ð´Ð²Ð° Ñ‡Ð¸ÑÐ»Ð°.

    :param a: ÐŸÐµÑ€Ð²Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾.
    :param b: Ð’Ñ‚Ð¾Ñ€Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾.
    :return: Ð¡ÑƒÐ¼Ð¼Ð° a Ð¸ b.
    test
    import numpy as np 
    qa = np.array([[1,2,3],[4,5,6]])
    b = np.transpose(qa) #or qa.T
    np.dot(b,qa) # multiplication
    b.shape #dimetions
    print(qa)
    take_col = qa[:,:2] # taking first two column
    reshape = qa.reshape(-1,1) # creating a new columns
    reshape
    """
    return a + b

def subtract(a, b):
    """
    Ð’Ñ‹Ñ‡Ð¸Ñ‚Ð°ÐµÑ‚ Ð²Ñ‚Ð¾Ñ€Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾.

    :param a: ÐŸÐµÑ€Ð²Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾.
    :param b: Ð’Ñ‚Ð¾Ñ€Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾.
    :return: Ð Ð°Ð·Ð½Ð¾ÑÑ‚ÑŒ a Ð¸ b.
    """
    return a - b

def multiply(a, b):
    """
    Ð£Ð¼Ð½Ð¾Ð¶Ð°ÐµÑ‚ Ð´Ð²Ð° Ñ‡Ð¸ÑÐ»Ð°.

    :param a: ÐŸÐµÑ€Ð²Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾.
    :param b: Ð’Ñ‚Ð¾Ñ€Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾.
    :return: ÐŸÑ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ a Ð¸ b.
    """
    return a * b

def divide(a, b):
    """
    Ð”ÐµÐ»Ð¸Ñ‚ Ð¿ÐµÑ€Ð²Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾ Ð½Ð° Ð²Ñ‚Ð¾Ñ€Ð¾Ðµ.

    :param a: ÐŸÐµÑ€Ð²Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾.
    :param b: Ð’Ñ‚Ð¾Ñ€Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾.
    :return: Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð´ÐµÐ»ÐµÐ½Ð¸Ñ a Ð½Ð° b.
    :raises ZeroDivisionError: Ð•ÑÐ»Ð¸ b Ñ€Ð°Ð²Ð½Ð¾ 0.
    """
    if b == 0:
        raise ZeroDivisionError("ÐÐµÐ»ÑŒÐ·Ñ Ð´ÐµÐ»Ð¸Ñ‚ÑŒ Ð½Ð° Ð½Ð¾Ð»ÑŒ!")
    return a / b


def numpym(a, b):
    """
    import numpy as np 
    qa = np.array([[1,2,3],[4,5,6]])
    b = np.transpose(qa) #or qa.T
    np.dot(b,qa) # multiplication
    b.shape #dimetions
    print(qa)
    take_col = qa[:,:2] # taking first two column
    reshape = qa.reshape(-1,1) # creating a new columns
    reshape
    """
    return a + b


def helpm(a, b):
    """
    all imports:
    one_m - basic 
    two_m - ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ, Ðº-means, ++, inertia, elbow, silhoette, convex, hierarchical, euclidian distance
    three_m - single complete and centroid linkage, agglomerative and divisive clustering
    four_m - sklearn, fit, predict, transform
    five_m - soft clustering, fuzzy, c-means, generalized inertia, eigenvectors, eigenvalues
    six_m - svd, norm, rank, threshold, pca, projections!
    seven_m - ICA
    eight_m - Anomaly detection, dbscan, iso forest, iso tree, hyper-parameters, NMF, components of it
    nine_m = ten_m
    ten_m - Text features, bag of words, word embedings, text preprocessing, stemming, lemmatization, tf-idf, n-gramns, topic-modeling, LDA

    Vocabulary
    Centroid: The mean of all points in a cluster.
    Linkage: The method used to determine the distance between clusters in hierarchical clustering.
    Orthonormal: Vectors that are both orthogonal (perpendicular) and normalized (length = 1).
    Orthogonal: Vectors that are perpendicular to each other (dot product = 0).
    Eigenvalues: Scalars that indicate the magnitude of the eigenvectors in PCA.
    Projection: Mapping data points onto a lower-dimensional space (e.g., PCA).
    Anomaly Score: A measure used in Isolation Forest to identify outliers.
    Non-Negative Matrix Factorization (NMF): A technique for factorizing a matrix into two nonï¿¾negative matrices.
    Topic Modeling: A technique for discovering abstract topics in a collection of documents (e.g., LDA).
    """     
    return a + b

def ten(a, b):
    """
        1. Text Features (Ð¢ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸)
        Ð¢ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ â€” ÑÑ‚Ð¾ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð² Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸. ÐŸÐ¾ÑÐºÐ¾Ð»ÑŒÐºÑƒ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹ ML Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ñ Ñ‡Ð¸ÑÐ»Ð°Ð¼Ð¸, Ñ‚ÐµÐºÑÑ‚ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð² Ñ‡Ð¸ÑÐ»Ð¾Ð²Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚.
        2. Bag of Words (ÐœÐµÑˆÐ¾Ðº ÑÐ»Ð¾Ð²)
        Bag of Words (BoW) â€” ÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð± Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°, Ð³Ð´Ðµ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ðµ ÑÐ»Ð¾Ð²Ð° Ð² Ñ‚ÐµÐºÑÑ‚Ðµ.
    ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚?
    Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ÑÑ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð²ÑÐµÑ… ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ»Ð¾Ð² Ð² Ñ‚ÐµÐºÑÑ‚Ð°Ñ….
    ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°, Ð³Ð´Ðµ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ€Ð°Ð· ÑÐ»Ð¾Ð²Ð¾ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¢ÐµÐºÑÑ‚Ñ‹:
    "I love cats"
    "I hate cats"
    Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ: ['I', 'love', 'hate', 'cats']
    Ð’ÐµÐºÑ‚Ð¾Ñ€Ñ‹:
    "I love cats" â†’ [1, 1, 0, 1]
    "I hate cats" â†’ [1, 0, 1, 1]
    ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹:
    Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐ»Ð¾Ð².
    ÐÐµ ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÑƒ (Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð²).

    3. Word Embeddings (Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð²)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    Word Embeddings â€” ÑÑ‚Ð¾ ÑÐ¿Ð¾ÑÐ¾Ð± Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð² Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð² Ð¼Ð½Ð¾Ð³Ð¾Ð¼ÐµÑ€Ð½Ð¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ, Ð³Ð´Ðµ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð±Ð»Ð¸Ð·ÐºÐ¸Ðµ ÑÐ»Ð¾Ð²Ð° Ð½Ð°Ñ…Ð¾Ð´ÑÑ‚ÑÑ Ð±Ð»Ð¸Ð·ÐºÐ¾ Ð´Ñ€ÑƒÐ³ Ðº Ð´Ñ€ÑƒÐ³Ñƒ.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹:
    Word2Vec.ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚?
    ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¼ ÐºÐ¾Ñ€Ð¿ÑƒÑÐµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð².
    ÐšÐ°Ð¶Ð´Ð¾Ðµ ÑÐ»Ð¾Ð²Ð¾ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 100-Ð¼ÐµÑ€Ð½Ð¾Ð³Ð¾).
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¡Ð»Ð¾Ð²Ð° "king" Ð¸ "queen" Ð±ÑƒÐ´ÑƒÑ‚ Ð±Ð»Ð¸Ð·ÐºÐ¸ Ð² Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ.
    Ð¡Ð»Ð¾Ð²Ð° "king" Ð¸ "apple" Ð±ÑƒÐ´ÑƒÑ‚ Ð´Ð°Ð»ÐµÐºÐ¸.
    4. Text Preprocessing (ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð°)
    ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð° â€” ÑÑ‚Ð¾ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð° Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°. Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚:
    Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð².
    Ð¡Ñ‚ÐµÐ¼Ð¼Ð¸Ð½Ð³ Ð¸Ð»Ð¸ Ð»ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ.
    ÐŸÑ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ðº Ð½Ð¸Ð¶Ð½ÐµÐ¼Ñƒ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ñƒ.
    Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸ Ð¸ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð².
    5. Stopwords (Ð¡Ñ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²Ð°)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    Ð¡Ñ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²Ð° â€” ÑÑ‚Ð¾ ÑÐ»Ð¾Ð²Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ Ð½ÐµÑÑƒÑ‚ ÑÐ¼Ñ‹ÑÐ»Ð¾Ð²Ð¾Ð¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "Ð¸", "Ð²", "Ð½Ð°").
    Ð—Ð°Ñ‡ÐµÐ¼ ÑƒÐ´Ð°Ð»ÑÑ‚ÑŒ?
    Ð£Ð¼ÐµÐ½ÑŒÑˆÐ°ÑŽÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ….
    Ð£Ð»ÑƒÑ‡ÑˆÐ°ÑŽÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚: "Ð¯ Ð¸Ð´Ñƒ Ð² Ð¼Ð°Ð³Ð°Ð·Ð¸Ð½"
    ÐŸÐ¾ÑÐ»Ðµ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²: "Ð¸Ð´Ñƒ Ð¼Ð°Ð³Ð°Ð·Ð¸Ð½"
    6. Stemming (Ð¡Ñ‚ÐµÐ¼Ð¼Ð¸Ð½Ð³)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    Ð¡Ñ‚ÐµÐ¼Ð¼Ð¸Ð½Ð³ â€” ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð²Ð° Ðº ÐµÐ³Ð¾ ÐºÐ¾Ñ€Ð½ÐµÐ²Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ðµ.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¡Ð»Ð¾Ð²Ð° "running", "runner", "runs" â†’ "run"
    7. Lemmatization (Ð›ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    Ð›ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ â€” ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð²Ð° Ðº ÐµÐ³Ð¾ ÑÐ»Ð¾Ð²Ð°Ñ€Ð½Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ðµ (Ð»ÐµÐ¼Ð¼Ðµ).
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¡Ð»Ð¾Ð²Ð° "running", "runner", "runs" â†’ "run"
    8. TF-IDF (Term Frequency-Inverse Document Frequency)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    TF-IDF â€” ÑÑ‚Ð¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¼ÐµÑ€Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÑÐ»Ð¾Ð²Ð° Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².
    ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚?
    TF (Term Frequency): Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð° ÑÐ»Ð¾Ð²Ð° Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ.
    IDF (Inverse Document Frequency): Ð›Ð¾Ð³Ð°Ñ€Ð¸Ñ„Ð¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°, ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‰ÐµÐ³Ð¾ ÑÐ»Ð¾Ð²Ð¾.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¡Ð»Ð¾Ð²Ð¾ "the" Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ñ‡Ð°ÑÑ‚Ð¾, Ð½Ð¾ Ð½Ðµ Ð²Ð°Ð¶Ð½Ð¾ (Ð½Ð¸Ð·ÐºÐ¸Ð¹ TF-IDF).
    Ð¡Ð»Ð¾Ð²Ð¾ "cat" Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ñ€ÐµÐ´ÐºÐ¾, Ð½Ð¾ Ð²Ð°Ð¶Ð½Ð¾ (Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¹ TF-IDF).
    Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð°:
    TF-IDF(t,d)=TF(t,d)Ã—log( DF(t)N )
    Ð³Ð´Ðµ:
    t â€” Ñ‚ÐµÑ€Ð¼Ð¸Ð½ (ÑÐ»Ð¾Ð²Ð¾).
    d â€” Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚.
    N â€” Ð¾Ð±Ñ‰ÐµÐµ Ñ‡Ð¸ÑÐ»Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².
    DF(t) â€” Ñ‡Ð¸ÑÐ»Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð², ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‰Ð¸Ñ… Ñ‚ÐµÑ€Ð¼Ð¸Ð½ t.
    9. n-Grams (n-Ð³Ñ€Ð°Ð¼Ð¼Ñ‹)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    n-Grams â€” ÑÑ‚Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¸Ð· n ÑÐ»Ð¾Ð² Ð¸Ð»Ð¸ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð².
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð‘Ð¸Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ (2-Ð³Ñ€Ð°Ð¼Ð¼Ñ‹): "I love", "love cats"
    Ð¢Ñ€Ð¸Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ (3-Ð³Ñ€Ð°Ð¼Ð¼Ñ‹): "I love cats"
    Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐ»Ð¾Ð².
    10. LDA (Latent Dirichlet Allocation)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    LDA â€” ÑÑ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´ Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ…. 
    ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ ÐºÐ°Ðº ÑÐ¼ÐµÑÑŒ Ñ‚ÐµÐ¼.
    ÐšÐ°Ð¶Ð´Ð°Ñ Ñ‚ÐµÐ¼Ð° â€” ÑÑ‚Ð¾ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð².
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¢ÐµÐ¼Ð° 1: "ÐºÐ¾ÑˆÐºÐ¸", "ÑÐ¾Ð±Ð°ÐºÐ¸", "Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ñ‹Ðµ"
    Ð¢ÐµÐ¼Ð° 2: "Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ", "Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹", "ÐºÐ¾Ð´"
    ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ:
    ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².
    ÐŸÐ¾Ð¸ÑÐº ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‚ÐµÐ¼.
    Description: LDA is a generative probabilistic model used for topic modeling. It assumes that
    documents are mixtures of topics and topics are mixtures of words.
    Steps:
    1. Initialize topic distributions for each document.
    2. Initialize word distributions for each topic.
    3. Iteratively update the distributions using Gibbs sampling or variational inference.
    """
    return a + b

def one(a, b):
    """
    ÐžÑÐ½Ð¾Ð²Ñ‹, Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° Ð¸ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ‹, Ð¾Ñ€Ñ‚Ð¾Ð³Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ.
    ÐžÑ€Ñ‚Ð¾Ð³Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð¿ÐµÑ€Ð¿ÐµÐ½Ð´Ð¸ÐºÑƒÐ»ÑÑ€Ð½Ñ‹ Ð´Ñ€ÑƒÐ³ Ð´Ñ€ÑƒÐ³Ñƒ, 
    Ð¸Ñ… Ð¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ñ€Ð°Ð²Ð½Ð¾ Ð½ÑƒÐ»ÑŽ ÐžÑ€Ñ‚Ð¾Ð³Ð¾Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° - ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð½Ð°Ñ, ÐµÑÐ»Ð¸ ÑƒÐ¼Ð½Ð¾Ð¶Ð¸Ñ‚ÑŒ ÐµÑ‘ Ð½Ð° Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ, Ñ‚Ð¾ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑÑ ÐµÐ´Ð¸Ð½Ð¸Ñ‡Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°
    # Ð•Ð´Ð¸Ð½Ð¸Ñ‡Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°:
    import numpy as np
    a = np.array([[1,0],[0,1]])
    #ÐšÐ°Ðº Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ numpy:
    #help(np.transpose)
    qa = np.array([[1,2,3],[4,5,6]])
    b = np.transpose(qa)
    qa, b
    np.dot(b,qa) # Ð£Ð¼Ð½Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†
    b.shape, qa.shape #ÑƒÐ·Ð½Ð°Ñ‚ÑŒ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ
    """
    return a + b

def two(a, b):
    """
    Ð“Ð»Ð°Ð²Ð° 2. ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ, Ðº-means, ++, inertia, elbow, silhoette, convex, hierarchical, euclidian distance
    ÐœÐµÑ‚Ð¾Ð´ Elbow (Ð»Ð¾ÐºÑ‚Ñ)
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² Ð² K-Means.
    Ð¡ÑƒÑ‚ÑŒ:
    Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÑÑƒÐ¼Ð¼Ñƒ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¾Ð² Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ Ð¾Ñ‚ Ñ‚Ð¾Ñ‡ÐµÐº Ð´Ð¾ Ð¸Ñ… Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ð° (Within-Cluster Sum of Squares, WCSS).
    Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð³Ñ€Ð°Ñ„Ð¸Ðº Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ WCSS Ð¾Ñ‚ Ñ‡Ð¸ÑÐ»Ð° ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² k.
    Ð˜Ñ‰ÐµÐ¼ "Ð»Ð¾ÐºÐ¾Ñ‚ÑŒ" â€” Ñ‚Ð¾Ñ‡ÐºÑƒ, Ð³Ð´Ðµ Ð¿Ñ€Ð¸Ñ€Ð¾ÑÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð·Ð°Ð¼ÐµÐ´Ð»ÑÐµÑ‚ÑÑ

    Ð˜Ð½ÐµÑ€Ñ†Ð¸Ñ â€” ÑÑ‚Ð¾ ÑÑƒÐ¼Ð¼Ð° ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¾Ð² Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ Ð²ÑÐµÑ… Ñ‚Ð¾Ñ‡ÐµÐº Ð´Ð¾ Ð¸Ñ… Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐµÐ³Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ð°.
    Ð§ÐµÐ¼ Ð¼ÐµÐ½ÑŒÑˆÐµ Ð¸Ð½ÐµÑ€Ñ†Ð¸Ñ, Ñ‚ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ (Ð½Ð¾ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¼Ð°Ð»Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¾Ð·Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ).
    Ð˜ÐÐµÑ€Ñ†Ð¸Ñ(Inertia) - ÑÑ‚Ð¾ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð½Ð¾Ðµ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¸Ñ‡Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ðµ!
    ÐºÑ€Ñ‡, Ð±ÐµÑ€Ñ‘Ð¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ‚ÐµÑ€ Ð¸ ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÑÑƒÐ¼Ð¼Ñƒ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¾Ð² Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð²ÑÐµÑ… Ñ‚Ð¾Ñ‡ÐµÐº Ð´Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ñ‹
    Ð·Ð°Ñ‚ÐµÐ¼ Ñ‚Ð°Ðº ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð´Ð»Ñ n ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð², Ð¸Ñ… Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ 5, 10 Ð¸ Ñ‚Ð´ Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ñ€Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ 

    elbow:
    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    X = np.random.rand(100, 2)

    # Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ WCSS Ð´Ð»Ñ Ñ€Ð°Ð·Ð½Ñ‹Ñ… k
    wcss = []
    for k in range(1, 11):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)  # inertia_ = ÑÑƒÐ¼Ð¼Ð° ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¾Ð² Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ Ð´Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ð¾Ð²

    # Ð Ð¸ÑÑƒÐµÐ¼ Ð³Ñ€Ð°Ñ„Ð¸Ðº
    plt.plot(range(1, 11), wcss, marker='o')
    plt.xlabel("ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² (k)")
    plt.ylabel("WCSS")
    plt.title("Elbow Method")
    plt.show()

    from sklearn.metrics import silhouette_score
    # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ KMeans Ñ 3 ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°Ð¼Ð¸
    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    # Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÑÐ¸Ð»ÑƒÑÑ‚
    sil_score = silhouette_score(X, labels)
    print(f"Silhouette Score: {sil_score:.3f}")

    Ð’ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ðµ Ð´ÑƒÐ¼Ð°ÑŽ ÑÑ‚Ð¾Ð¸Ñ‚ Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ Ñ Ð´Ð¸ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¸ ÑÐ²ÐºÐ»Ð¸Ð´Ð°, Ð±Ð»Ð°Ð³Ð¾ ÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑˆÑ‘Ð» Ð¸ ÑÑ‚Ð¾ Ñ‡Ð°ÑÑ‚ÑŒ Ð¾Ð´Ð½Ð° Ð¸Ð· ÑÐ°Ð¼Ñ‹Ñ… Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð˜ Ñ‚Ð°Ðº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ð°Ð¹Ñ‚Ð¸ Ð´Ð¸ÑÑ‚Ð°Ñ†Ð¸ÑŽ Ð¼ÐµÐ¶Ð´Ñƒ 
    ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ð°Ð¼Ð¸ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð´Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼ [a1,a2] Ð¸ [b1,b2] Ð½Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ dist = math.sqrt((([a1-b1])**2+([a2-b2])**2) ÐºÑ€Ñ‡ Ð½Ð°Ð¹Ñ‚Ð¸ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ñƒ Ð¼ÐµÐ¶Ð´Ñƒ Ð¿ÐµÑ€Ð²Ñ‹Ð¼Ð¸ 
    Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð², Ð²Ñ‹Ð²Ð¾Ð·Ð²ÐµÑÑ‚Ð¸ Ð² ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚, ÑÐ»Ð¾Ð¶Ð¸Ñ‚ÑŒ Ñ Ñ€Ð°Ð·Ð½Ð¸Ñ†ÐµÐ¹ Ð²Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð¸ Ð²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°, 
    Ñ‚Ð¾Ð¶Ðµ Ð² ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚, Ð¸ Ñ‚Ð´, Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð´Ð¾Ð¹Ð´Ñ‘Ð¼ Ð´Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ, Ð° Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð²ÑÑŽ ÑÑ‚Ñƒ ÑÑƒÐ¼Ð¼Ñƒ Ð²Ð·ÑÑ‚ÑŒ Ð² ÐºÐ¾Ñ€ÐµÐ½ÑŒ. Ð’Ð¾Ñ‚ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ ÑÐ°Ð¼Ð¾Ð¿Ð¸ÑÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€:

    import numpy as np
    import math
    a,b,c,d = [1, 2, 3], [0, 3, 3], [4, 2, 9], [2, 2, 2]
    def euc(first):    
        origin = [0,0,0]
        len_first = len(first) # check size of vectors
        len_origin = len(origin) 
        if len_first > len_origin:
            for i in range(len_first - len_origin):
                origin = origin+[0]
        #print(origin)
        if len_first < len_origin:
            for i in range(len_origin - len_first ):
                first = first+[0]
        #print(first)
        sum = 0
        for i in range(len_first):
            sum = sum + (first[i]-origin[i])**2
        final_sum = math.sqrt(sum)     
        #print(final_sum)
        return(final_sum)

    vectors = a,b,c,d
    def find(vectors):
        dist=[]
        for vector in range(len(vectors)):
            dist = dist + [euc(vectors[vector])]
        min_dist = min(dist)  
        min_arg = dist.index(min_dist)
        print(f"{vectors[min_arg]} - the closest vector, the distance is {min_dist}")
        print()
    find(vectors)  


    Ñ‚ÑƒÑ‚ Ð¿Ð¾ ÑÑƒÑ‚Ð¸ Ð¿ÐµÑ€Ð²Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ - ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð´Ð»Ð¸Ð½Ñ‹ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð², Ð° Ð·Ð°Ñ‡ÐµÐ¼ Ð½Ð°Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ðµ Ð´Ð¸ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ 
    Ð¿ÐµÑ€Ð²Ñ‹Ð¼ Ð¸ Ð²Ñ‚Ð¾Ñ€Ñ‹Ð¼ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð¼. Ð’Ñ‚Ð¾Ñ€Ð°Ñ Ð¶Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð±ÐµÑ€Ñ‘Ñ‚ Ð²ÑÐµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°, Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð´Ð¸ÑÑ‚Ð°Ð½Ñ†Ð¸ÑŽ, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ min() Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ
    ÐµÑÑ‚ÑŒ ÐµÑ‰Ñ‘ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¸Ð· numpy, Ð´Ð»Ñ Ð½Ð°Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ðµ Ð´Ð¸ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¸ Ð² Ð¾Ð´Ð½Ñƒ ÑÑ‚Ñ€Ð¾Ñ‡ÐºÑƒ:

    first, origin = np.array([1, 2, 3]), np.array([0, 3, 3])
    sum = np.linalg.norm(first - origin, axis = 0) # Ð»Ð¸Ð½Ð°Ð»Ð³.Ð½Ð¾Ñ€Ð¼, Ð½Ð¾ Ð¿ÐµÑ€ÐµÐ´ ÑÑ‚Ð¸Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð½ÑƒÑ‚ÑŒ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° Ð² np.array
    sum

    ÐšÐ°Ðº ÑƒÑÑ‚Ñ€Ð¾ÐµÐ½ Ðš-mean, Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼:
    Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÐºÐ¾Ð»-Ð²Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²
    Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ñ‹
    Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð´Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´ Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ðº ÐºÐ°ÐºÐ¾Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ðµ Ñ‚Ð° Ð¸Ð»Ð¸ Ð¸Ð½Ð°Ñ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð¸Ñ‚
    ÐŸÐ¾ÑÐ»Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ðº Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ðµ Ð²Ñ‹ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ€ÐµÐ´Ð¸ ÑÑ‚Ð¸Ñ… Ñ‚Ð¾Ñ‡ÐµÐº Ð½Ð¾Ð²ÑƒÑŽ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ñƒ (ÑÐºÐ»Ð°Ð´Ñ‹Ð²Ð°Ñ Ð²ÑÐµ Ð¿Ð¾ Ð¾ÑÐ¸ Ñ…, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ð´ÐµÐ»Ñ Ð½Ð° ÐºÐ¾Ð»-Ð²Ð¾ Ñ‚Ð¾Ñ‡ÐµÐº, Ð·Ð°Ñ‚ÐµÐ¼ Ñ‚Ð°ÐºÐ¶Ðµ Ð´Ð»Ñ y Ð¸ Ð¿Ð¾ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¼ Ð¾ÑÑÐ¼, Ð² Ð¸Ñ‚Ð¾Ð³Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð½Ð¾Ð²ÑƒÑŽ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ñ‹ Ð´Ð»Ñ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ð°)
    Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐµÐ¼ 3-4 Ð¿Ð¾ÐºÐ° Ð¸Ð»Ð¸ Ð½Ðµ ÐºÐ¾Ð½Ñ‡Ð°Ñ‚ÑÑ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð»Ð¸Ð±Ð¾ Ð¿Ð¾ÐºÐ° Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ñ‹ Ð½Ðµ Ð¿ÐµÑ€ÐµÑÑ‚Ð°Ð»Ð¸ Ð´Ð²Ð¸Ð³Ð°Ñ‚ÑŒÑÑ

    K++: 
    Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾ Ð¸Ð· Ð²ÑÐµÑ… Ñ‚Ð¾Ñ‡ÐµÐº.
    Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‚Ð¾Ñ‡ÐºÐ¸  Ð½Ð°Ñ…Ð¾Ð´Ð¸Ð¼ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð´Ð¾ Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐµÐ³Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ð°:
    Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ðµ Ñ ÐµÐ²ÐºÐ»Ð¸Ð´Ð¾Ð²Ð¾Ð¹ Ð´Ð¸ÑÑ‚Ð°Ð½Ñ†Ð¸ÐµÐ¹
    ÐµÐ¼ Ð´Ð°Ð»ÑŒÑˆÐµ Ñ‚Ð¾Ñ‡ÐºÐ° Ð¾Ñ‚ Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐµÐ³Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ð°, Ñ‚ÐµÐ¼ Ð²Ñ‹ÑˆÐµ ÐµÑ‘ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð±Ñ‹Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¼ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ð¾Ð¼.
    Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾, Ð½Ð¾ Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹ 
    ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€ÑÐµÐ¼ ÑˆÐ°Ð³Ð¸ 2-4, Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð¾ k Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ð¾Ð².

    from sklearn.cluster import KMeans
    #KMeans(    n_clusters=8,    *,    init='k-means++',    n_init='auto',    max_iter=300,     random_state=None,    copy_x=True,    algorithm='lloyd',)
    import numpy as np
    X = np.random.uniform(0, 10, (100, 2))
    kmeans = KMeans(n_clusters=5, random_state=0, n_init="auto").fit(X)
    kmeans.labels_
    kmeans.predict([[0, 0], [12, 3]])
    kmeans.cluster_centers_
    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')
    plt.title("ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ")
    plt.show()

    ÐÐ»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð°Ð³Ð»Ð¾Ð¼ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ (hierarchical) ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    ÐÐ°Ñ‡Ð°Ð»Ð¾: ÐºÐ°Ð¶Ð´Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ° â€” ÑÑ‚Ð¾ ÑÐ²Ð¾Ð¹ ÐºÐ»Ð°ÑÑ‚ÐµÑ€.
    ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ: Ð½Ð°Ñ…Ð¾Ð´Ð¸Ð¼ Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐ¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹ Ð¸ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð¸Ñ….
    ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€ÑÐµÐ¼ Ð´Ð¾ Ñ‚ÐµÑ… Ð¿Ð¾Ñ€, Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð¾ÑÑ‚Ð°Ð½ÐµÑ‚ÑÑ Ð½ÑƒÐ¶Ð½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð².
    Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ÑÑ Ð´ÐµÐ½Ð´Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð° (Ð´ÐµÑ€ÐµÐ²Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²).
    ÐœÐµÑ‚Ð¾Ð´Ñ‹ Ð¸Ð·Ð¼ÐµÑ€ÐµÐ½Ð¸Ñ "Ð±Ð»Ð¸Ð·Ð¾ÑÑ‚Ð¸" ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² (linkage):
    Single linkage â€” Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼ÐµÐ¶Ð´Ñƒ Ñ‚Ð¾Ñ‡ÐºÐ°Ð¼Ð¸ Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð².
    Complete linkage â€” Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼ÐµÐ¶Ð´Ñƒ Ñ‚Ð¾Ñ‡ÐºÐ°Ð¼Ð¸.
    Average linkage â€” ÑÑ€ÐµÐ´Ð½ÐµÐµ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼ÐµÐ¶Ð´Ñƒ Ð²ÑÐµÐ¼Ð¸ Ñ‚Ð¾Ñ‡ÐºÐ°Ð¼Ð¸.
    Ward linkage â€” Ð¼Ð¸Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð´Ð¸ÑÐ¿ÐµÑ€ÑÐ¸Ð¸ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð².

    ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð² AgglomerativeClustering (sklearn)
    n_clusters â€” ÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ.
    linkage â€” Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ ('ward', 'single', 'complete', 'average').
    affinity â€” Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ° Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ (Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ 'euclidean').

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.cluster import AgglomerativeClustering
    from scipy.cluster.hierarchy import dendrogram, linkage
    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    X = np.array([[1, 2], [2, 3], [3, 4], [8, 7], [8, 8], [9, 7]])
    # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð°Ð³Ð»Ð¾Ð¼ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½ÑƒÑŽ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸ÑŽ (2 ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°, Ð¼ÐµÑ‚Ð¾Ð´ Ward)
    clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')
    labels = clustering.fit_predict(X)
    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')
    plt.title("Agglomerative Clustering")
    plt.show()
    # Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð´ÐµÐ½Ð´Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ
    linked = linkage(X, method='ward')
    plt.figure(figsize=(8, 5))
    dendrogram(linked, labels=range(len(X)))
    plt.title("Dendrogram")
    plt.show()

    ÐºÐ°Ðº Ð½Ð°Ð¹Ñ‚Ð¸ ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° Ð¸ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
    test = np.array([[1, 2, 3], [0, 3, 3], [4, 2, 9]])
    eigenvalues, eug = np.linalg.eig(test)
    eigenvalues, eug
    """
    return a + b

def three(a, b):
    """
    Ð˜ Ñ‚Ð°Ðº, Ð»Ð¸Ð½ÐºÐ°Ð´Ð¶Ð¸ - ÑÑ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ñ€Ð°ÑÑÑ‡Ñ‘Ñ‚Ð° Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°Ð¼Ð¸
    Agglomerative: ÑÐ½Ð¸Ð·Ñƒ Ð²Ð²ÐµÑ€Ñ…, Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹. Ð§Ð°Ñ‰Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ. Divisive: ÑÐ²ÐµÑ€Ñ…Ñƒ Ð²Ð½Ð¸Ð·, Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐµÑ‚ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹. Ð ÐµÐ´ÐºÐ¾ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ÑÑ Ð¸Ð·-Ð·Ð° ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸.
    """
    return a + b

def four(a, b):
    """
    sklearn, fit, predict, transform

    fit: ÐžÐ±ÑƒÑ‡Ð°ÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ….
    predict: Ð”ÐµÐ»Ð°ÐµÑ‚ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð½Ð° Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….
    transform: ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ).
    Ð—Ð°Ð´Ð°Ñ‡Ð°: Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ….
    Ð¨Ð°Ð³Ð¸: Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ.
    ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ PCA.
    ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ.
    kmeans:
    Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ. ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ K-means. Ð”ÐµÐ»Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ.

    from sklearn.cluster import KMeans
    import numpy as np
    import matplotlib.pyplot as plt
    # 1. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])
    # 2. ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ K-means
    kmeans = KMeans(n_clusters=2, random_state=0)
    kmeans.fit(X)  # fit â€” Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
    # 3. Ð”ÐµÐ»Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
    labels = kmeans.predict(X)  # predict â€” Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹
    print("ÐœÐµÑ‚ÐºÐ¸ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²:", labels)
    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x')  # Ð¦ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ñ‹
    plt.title("K-means ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ")
    plt.show()

    Ð¸Ð»Ð¸ ÐŸÐ¡Ð

    from sklearn.decomposition import PCA
    import numpy as np
    import matplotlib.pyplot as plt
    # 1. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
    # 2. ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ PCA
    pca = PCA(n_components=1)
    pca.fit(X)  # fit â€” Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
    # 3. ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    X_transformed = pca.transform(X)  # transform â€” Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    print("ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ:\n", X_transformed)
    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    plt.scatter(X[:, 0], X[:, 1], label='Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ')
    plt.scatter(X_transformed, [0] * len(X_transformed), label='ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ')
    plt.legend()
    plt.title("PCA")
    plt.show()

    """
    return a + b

def five(a, b):
    """
     soft clustering, fuzzy, c-means, generalized inertia, eigenvectors, eigenvalues

    NMF: Ð’ Non-Negative Matrix Factorization (NMF), W H â€” ÑÑ‚Ð¾ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ‹, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ€Ð°Ð·Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð¸ÑÑ…Ð¾Ð´Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° V, Ð¿Ñ€Ð¸Ñ‡ÐµÐ¼ Ð²ÑÐµ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ 
    ÑÑ‚Ð¸Ñ… Ð¼Ð°Ñ‚Ñ€Ð¸Ñ† Ð½ÐµÐ¾Ñ‚Ñ€Ð¸Ñ†Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹. Ð Ð°Ð·Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð²Ñ‹Ð´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ (Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ñ‹Ðµ) ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ñ‡Ñ‚Ð¾ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾ Ð² Ñ‚Ð°ÐºÐ¸Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…, 
    ÐºÐ°Ðº Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð»Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð². W Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð±Ð¾Ð¹ "Ñ‚ÐµÐ¼Ñ‹" Ð¸Ð»Ð¸ "Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹". Ð’ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ñ‚ÐµÐºÑÑ‚Ð°, 
    Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, ÑÑ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°, Ð³Ð´Ðµ ÐºÐ°Ð¶Ð´Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° W Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐºÑ€Ñ‹Ñ‚ÑƒÑŽ Ñ‚ÐµÐ¼Ñƒ (Ð¸Ð»Ð¸ ÑÐºÑ€Ñ‹Ñ‚ÑƒÑŽ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÑƒ), Ð° ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÑ‚Ð¾Ð»Ð±ÐµÑ† â€” Ð²ÐµÑ,
    Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¼ ÑÑ‚Ð° Ñ‚ÐµÐ¼Ð° ÑƒÑ‡Ð°ÑÑ‚Ð²ÑƒÐµÑ‚ Ð² Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¾Ð±ÑŠÐµÐºÑ‚Ð°Ñ… (Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ…). ÐšÐ°Ð¶Ð´Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° W ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ñ‚Ð¾Ð¼Ñƒ, ÐºÐ°Ðº ÐºÐ°Ð¶Ð´Ð°Ñ Ñ‚ÐµÐ¼Ð° Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð°
    Ð² Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° H (Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ ð‘ŸÃ—ð‘›): H Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð±Ð¾Ð¹ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸ÑŽ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚ Ð¸Ð»Ð¸ Ð²ÐµÑÐ¾Ð²Ñ‹Ðµ ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² 
    (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²). Ð’ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ñ‚ÐµÐºÑÑ‚Ð° ÑÑ‚Ð¾ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°, Ð³Ð´Ðµ ÐºÐ°Ð¶Ð´Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° H ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð²ÐµÑÑƒ Ñ‚ÐµÐ¼Ñ‹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¾Ð±ÑŠÐµÐºÑ‚Ð°

    Eigenvectors Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ð¾ÑÐ¾Ð±Ñ‹Ðµ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ, Ð² ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€, Ð° Eigenvalues Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‚, Ð½Ð°ÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐ¸Ð»ÑŒÐ½Ð¾.

    Fuzzy C-Means â€” ÑÑ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸, Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ð¹ Ð½Ð° K-Means, Ð½Ð¾ Ð²Ð¼ÐµÑÑ‚Ð¾ Ñ‡Ñ‘Ñ‚ÐºÐ¾Ð³Ð¾ Ð´ÐµÐ»ÐµÐ½Ð¸Ñ
    Ñ‚Ð¾Ñ‡ÐµÐº Ð½Ð° ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹ Ð¾Ð½ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‚Ð¾Ñ‡ÐºÐµ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð°Ñ‚ÑŒ ÑÑ€Ð°Ð·Ñƒ Ðº Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ð¼ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°Ð¼ Ñ Ñ€Ð°Ð·Ð½Ð¾Ð¹ ÑÑ‚ÐµÐ¿ÐµÐ½ÑŒÑŽ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸. ÐÐ»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Fuzzy C-Means (ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸)
    1.Ð—Ð°Ð´Ð°Ñ‘Ð¼ Ñ‡Ð¸ÑÐ»Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² k Ð¸ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ñ… Ñ†ÐµÐ½Ñ‚Ñ€Ñ‹.
    2.Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼, Ð½Ð°ÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÐºÐ°Ð¶Ð´Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ° Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð¸Ñ‚ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñƒ. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚ÐµÐ¿ÐµÐ½ÑŒ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸ (membership): Ñ‡ÐµÐ¼ Ð±Ð»Ð¸Ð¶Ðµ Ñ‚Ð¾Ñ‡ÐºÐ° Ðº Ñ†ÐµÐ½Ñ‚Ñ€Ñƒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°, Ñ‚ÐµÐ¼ ÑÐ¸Ð»ÑŒÐ½ÐµÐµ Ð¾Ð½Ð° Ðº Ð½ÐµÐ¼Ñƒ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÑÑ.
    3.ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ†ÐµÐ½Ñ‚Ñ€Ñ‹ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²: ÐÐ¾Ð²Ñ‹Ðµ Ñ†ÐµÐ½Ñ‚Ñ€Ñ‹ ÑÑ‡Ð¸Ñ‚Ð°ÑŽÑ‚ÑÑ ÐºÐ°Ðº Ð²Ð·Ð²ÐµÑˆÐµÐ½Ð½Ð¾Ðµ ÑÑ€ÐµÐ´Ð½ÐµÐµ Ð²ÑÐµÑ… Ñ‚Ð¾Ñ‡ÐµÐº, Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ Ð¸Ñ… ÑÑ‚ÐµÐ¿ÐµÐ½Ð¸ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸.
    4.ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€ÑÐµÐ¼ ÑˆÐ°Ð³Ð¸ 2â€“3, Ð¿Ð¾ÐºÐ° Ñ†ÐµÐ½Ñ‚Ñ€Ñ‹ Ð½Ðµ ÑÑ‚Ð°Ð±Ð¸Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð¸Ð»Ð¸ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ð½Ð¾Ð²ÑÑ‚ÑÑ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸.     

    import numpy as np
    import skfuzzy as fuzz
    import matplotlib.pyplot as plt
    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ðµ Ñ‚Ð¾Ñ‡ÐºÐ¸
    X = np.random.rand(100, 2)
    # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Fuzzy C-Means Ñ 2 ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°Ð¼Ð¸
    cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(X.T, c=2, m=2, error=0.005, maxiter=1000)
    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒ Ñ‚Ð¾Ñ‡ÐµÐº ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°Ð¼
    max_membership = np.max(u, axis=0)  # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ ÑÑ‚ÐµÐ¿ÐµÐ½ÑŒ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‚Ð¾Ñ‡ÐºÐ¸
    cluster_labels = np.argmax(u, axis=0)  # ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ ÐºÐ»Ð°ÑÑ‚ÐµÑ€ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‚Ð¾Ñ‡ÐºÐ¸
    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ "Ð½ÐµÐ¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½Ð½Ñ‹Ðµ" Ñ‚Ð¾Ñ‡ÐºÐ¸ (ÐµÑÐ»Ð¸ max_membership < 0.6)
    uncertain_points = max_membership < 0.6
    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    plt.scatter(X[~uncertain_points, 0], X[~uncertain_points, 1], c=cluster_labels[~uncertain_points], cmap='coolwarm', alpha=0.7, label="Ð§Ñ‘Ñ‚ÐºÐ¸Ðµ Ñ‚Ð¾Ñ‡ÐºÐ¸")
    plt.scatter(X[uncertain_points, 0], X[uncertain_points, 1], c='green', marker='o', edgecolors='black', label="Ð¡Ð¿Ð¾Ñ€Ð½Ñ‹Ðµ Ñ‚Ð¾Ñ‡ÐºÐ¸")  # Ð¡Ð¿Ð¾Ñ€Ð½Ñ‹Ðµ Ñ‚Ð¾Ñ‡ÐºÐ¸
    plt.scatter(cntr[:, 0], cntr[:, 1], c='black', marker='x', s=200, label="Ð¦ÐµÐ½Ñ‚Ñ€Ñ‹ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²")
    plt.legend()
    plt.show()

    """
    return a + b

def six(a, b):
    """
    Ð´Ð¾ SVD Ð¸ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾
    import numpy as np
    # ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° A (3x2)
    A = np.array([
        [3, 2],
        [2, 3],
        [1, 0]
    ])
    # SVD Ñ€Ð°Ð·Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ
    U, Sigma, Vt = np.linalg.svd(A)
    print("U:\n", U)
    print("Sigma:\n", Sigma)  # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ñ‚ 1D Ð¼Ð°ÑÑÐ¸Ð² Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ïƒ
    print("V^T:\n", Vt)
    s_matrix = np.zeros((3,2))
    for i in range(Sigma.size):
        s_matrix[i,i]=Sigma[i]
    #
    t = U@s_matrix@Vt
    s_matrix,t.round()

    PCA:
    Description: A dimensionality reduction technique that projects data onto orthogonal axes (principal
    components) that capture the most variance.
    Steps:
    1. Compute the mean of the data.
    2. Subtract the mean from each observation.
    3. Compute the covariance matrix.
    4. Perform eigenvalue decomposition on the covariance matrix.
    5. Select the top k eigenvectors (principal components).

    from sklearn.decomposition import PCA
    import numpy as np
    import matplotlib.pyplot as plt
    # 1. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
    # 2. ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ PCA
    pca = PCA(n_components=1)
    pca.fit(X)  # fit â€” Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
    # 3. ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    X_transformed = pca.transform(X)  # transform â€” Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    print("ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ:\n", X_transformed)
    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    plt.scatter(X[:, 0], X[:, 1], label='Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ')
    plt.scatter(X_transformed, [0] * len(X_transformed), label='ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ')
    plt.legend()
    plt.title("PCA")
    plt.show()

    Projection b on two main components U:
    U = np.array([[2,0,-2,0],[0,1,0,0],[2,0,2,0],[0,0,0,1]])
    U2 = U [:,:2]
    b = np.array([4,3,2,1]).reshape(-1,1)
    U2@U2.T@b
    """
    return a + b

def seven(a, b):
    """
    Ð§Ñ‚Ð¾ Ð´ÐµÐ»Ð°ÐµÑ‚ ICA?
    ICA Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð¸Ð· Ð¼Ð½Ð¾Ð³Ð¾Ð¼ÐµÑ€Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…. 
    ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð² Ð·Ð°Ð´Ð°Ñ‡Ðµ Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°Ð¼Ð¸ Ð·Ð²ÑƒÐºÐ° (ÑÐ»ÑƒÑˆÐ°ÐµÐ¼Ð°Ñ Ð¼ÑƒÐ·Ñ‹ÐºÐ°, Ñ€ÐµÑ‡ÑŒ Ð¸ Ñ‚.Ð´.), 
    ICA Ð¼Ð¾Ð¶ÐµÑ‚ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ€Ð°Ð·Ð´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÐ¼ÐµÑÑŒ ÑÑ‚Ð¸Ñ… Ð·Ð²ÑƒÐºÐ¾Ð² Ð² Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ (ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ Ð½Ðµ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ ÑÐ¸Ð»ÑŒÐ½Ð¾ Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ).
    Ð§Ñ‚Ð¾ Ð¿Ñ€Ð¾Ð¸ÑÑ…Ð¾Ð´Ð¸Ñ‚ Ð² fit_transform?
    ÐšÐ¾Ð³Ð´Ð° Ñ‚Ñ‹ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑˆÑŒ Ð¼ÐµÑ‚Ð¾Ð´ fit_transform, Ñ‚Ñ‹ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑˆÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ICA Ð½Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð° Ð·Ð°Ñ‚ÐµÐ¼ ÑÑ€Ð°Ð·Ñƒ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑˆÑŒ ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ 
    Ð² Ð½Ð¾Ð²Ð¾Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð¾, Ð² ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð² Ð²Ð¸Ð´Ðµ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚.

    ICM steps: 
    ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ ICA Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸
    1) Ð”ÐµÐ»Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑƒÐ´Ð¾Ð±Ð½Ñ‹Ð¼Ð¸ â€” Ð²Ñ‹Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÑÑ€ÐµÐ´Ð½ÐµÐµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ†ÐµÐ½Ñ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ñ… Ð²Ð¾ÐºÑ€ÑƒÐ³ Ð½ÑƒÐ»Ñ.
    2) Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÐºÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ð¸ â€” Ð´ÐµÐ»Ð°ÐµÐ¼ Ñ‚Ð°Ðº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ðµ Ð·Ð°Ð²Ð¸ÑÐµÐ»Ð¸ Ð´Ñ€ÑƒÐ³ Ð¾Ñ‚ Ð´Ñ€ÑƒÐ³Ð° (ÑÐ¶Ð¸Ð¼Ð°ÐµÐ¼ Ð¸Ñ… Ð² ÑÑ„ÐµÑ€Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ñ„Ð¾Ñ€Ð¼Ñƒ, ÐºÐ°Ðº Ð² PCA).
    3) Ð˜Ñ‰ÐµÐ¼ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ â€” Ð¿Ñ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð½Ð°Ð¹Ñ‚Ð¸ Ñ‚Ð°ÐºÐ¸Ðµ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ, Ð³Ð´Ðµ ÑÐ¸Ð³Ð½Ð°Ð»Ñ‹ ÑÑ‚Ð°Ð½Ð¾Ð²ÑÑ‚ÑÑ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ñ€Ð°Ð·Ð½Ñ‹Ð¼Ð¸ Ð¸ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ð¼Ð¸.
    4) Ð Ð°Ð·Ð´ÐµÐ»ÑÐµÐ¼ Ð¸Ñ… â€” Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñƒ Ð½Ð°Ñ ÐµÑÑ‚ÑŒ Ð½Ð°Ð±Ð¾Ñ€ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð±Ñ‹Ð»Ð¸ ÑÐ¼ÐµÑˆÐ°Ð½Ñ‹ Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð¼Ð¾Ð¶Ð½Ð¾ Ñ€Ð°Ð·Ð´ÐµÐ»Ð¸Ñ‚ÑŒ 
    Ð·Ð²ÑƒÐºÐ¸ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð»ÑŽÐ´ÐµÐ¹, Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ñ… Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾). 

    from sklearn.decomposition import FastICA

    ica = FastICA(n_components = 2)
    S = ica.fit_transform(X)
    S

    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (ÑÐ¼ÐµÑÑŒ Ð´Ð²ÑƒÑ… Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ñ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²)
    # Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº 1 Ð¸ Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº 2
    S1 = np.sin(2 * np.pi * 0.1 * np.arange(2000))
    S2 = np.sign(np.sin(2 * np.pi * 0.05 * np.arange(2000)))

    # Ð¡Ð¼ÐµÑˆÐ¸Ð²Ð°ÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸
    S = np.c_[S1, S2]
    A = np.array([[1, 1], [0.5, 2]])  # ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° ÑÐ¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ñ
    X = S.dot(A.T)  # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ð°Ñ ÑÐ¼ÐµÑÑŒ
    # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ ICA Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚
    ica = FastICA(n_components=2)
    S_ = ica.fit_transform(X)  # Ð’ S_ Ð¼Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹
    print(S1)
    print(S_)  # (2000, 2), 2 Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°
    """

    return a + b

def eight(a, b):
    """
    Anomaly detection, dbscan, iso forest, iso tree, hyper-parameters, NMF, components of it
    DBSCAN - Ð´Ð²Ð° Ð³Ð»Ð°Ð²Ð½Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°, 1)Ñ€Ð°Ð´Ð¸ÑƒÑ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸ 2) Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾ Ñ‚Ð¾Ñ‡ÐµÐº Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€
    Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Isolation Forest? Isolation Forest â€” ÑÑ‚Ð¾ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹. 
    ÐžÐ½ Ð¾ÑÐ½Ð¾Ð²Ð°Ð½ Ð½Ð° Ð¸Ð´ÐµÐµ, Ñ‡Ñ‚Ð¾ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¸ â€” ÑÑ‚Ð¾ Ñ€ÐµÐ´ÐºÐ¸Ðµ Ð¸ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð°ÑŽÑ‰Ð¸ÐµÑÑ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ "Ð¸Ð·Ð¾Ð»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ" (Ð¾Ñ‚Ð´ÐµÐ»Ð¸Ñ‚ÑŒ) Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ, Ñ‡ÐµÐ¼ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ.

    #Isolation forest:
    from sklearn.ensemble import IsolationForest
    import numpy as np
    import matplotlib.pyplot as plt
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0], [5, 3]])
    # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Isolation Forest
    iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)
    iso_forest.fit(X)
    # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹
    predictions = iso_forest.predict(X)
    print("ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ (1 = Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ°, -1 = Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ñ):", predictions)
    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    plt.scatter(X[:, 0], X[:, 1], c=predictions, cmap='viridis')
    plt.title("Isolation Forest")
    plt.show()
    #ÐžÐ±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ:
    #ÐœÑ‹ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ Ð´Ð²ÑƒÐ¼Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸.
    #Isolation Forest Ð¿Ð¾Ð¼ÐµÑ‚Ð¸Ð» Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¸ (Ñ‚Ð¾Ñ‡ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð´Ð°Ð»ÐµÐºÐ¸ Ð¾Ñ‚ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼Ð°ÑÑÑ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…).
    #ÐÐ° Ð³Ñ€Ð°Ñ„Ð¸ÐºÐµ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¸ Ð²Ñ‹Ð´ÐµÐ»ÐµÐ½Ñ‹ Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ñ†Ð²ÐµÑ‚Ð¾Ð¼.

    NMF - algorithm:
    1) initialize W 2) compute H using least squares (projection) and replace any negative values with 0 
        3) compute W using lest... 4) repeat steps 2 and 3 until convergence
    two hyper-parameters: 1)n_components(number of columns in W and number of rows in H)
    2) randome state(seed)

    NMF: Ð’ Non-Negative Matrix Factorization (NMF), 
    W H â€” ÑÑ‚Ð¾ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ‹, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ€Ð°Ð·Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð¸ÑÑ…Ð¾Ð´Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° V, Ð¿Ñ€Ð¸Ñ‡ÐµÐ¼ Ð²ÑÐµ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ ÑÑ‚Ð¸Ñ… Ð¼Ð°Ñ‚Ñ€Ð¸Ñ† Ð½ÐµÐ¾Ñ‚Ñ€Ð¸Ñ†Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹. 
    Ð Ð°Ð·Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð²Ñ‹Ð´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ (Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ñ‹Ðµ) ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ñ‡Ñ‚Ð¾ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾ Ð² Ñ‚Ð°ÐºÐ¸Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…,
    ÐºÐ°Ðº Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð»Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð².
    W Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð±Ð¾Ð¹ "Ñ‚ÐµÐ¼Ñ‹" Ð¸Ð»Ð¸ "Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹".
    Ð’ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ñ‚ÐµÐºÑÑ‚Ð°, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, ÑÑ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°, Ð³Ð´Ðµ ÐºÐ°Ð¶Ð´Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° 
    W Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐºÑ€Ñ‹Ñ‚ÑƒÑŽ Ñ‚ÐµÐ¼Ñƒ (Ð¸Ð»Ð¸ ÑÐºÑ€Ñ‹Ñ‚ÑƒÑŽ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÑƒ), Ð° ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÑ‚Ð¾Ð»Ð±ÐµÑ† â€” Ð²ÐµÑ, 
    Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¼ ÑÑ‚Ð° Ñ‚ÐµÐ¼Ð° ÑƒÑ‡Ð°ÑÑ‚Ð²ÑƒÐµÑ‚ Ð² Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¾Ð±ÑŠÐµÐºÑ‚Ð°Ñ… (Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ…).
    ÐšÐ°Ð¶Ð´Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° W ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ñ‚Ð¾Ð¼Ñƒ, ÐºÐ°Ðº ÐºÐ°Ð¶Ð´Ð°Ñ Ñ‚ÐµÐ¼Ð° Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð² Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….
    ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° H (Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ ð‘ŸÃ—ð‘›):
    H Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð±Ð¾Ð¹ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸ÑŽ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚ Ð¸Ð»Ð¸ Ð²ÐµÑÐ¾Ð²Ñ‹Ðµ ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²).
    Ð’ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ñ‚ÐµÐºÑÑ‚Ð° ÑÑ‚Ð¾ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°, Ð³Ð´Ðµ ÐºÐ°Ð¶Ð´Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° H ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð²ÐµÑÑƒ Ñ‚ÐµÐ¼Ñ‹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¾Ð±ÑŠÐµÐºÑ‚Ð° 

    X (m Ã— n) â€” Ð¸ÑÑ…Ð¾Ð´Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÑ… Ð¸ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ…).
    ð‘Š
    W (m Ã— k) â€” Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° "Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð¾Ð²" (Ð¾Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚, ÐºÐ°Ðº Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ ÑÐ²ÑÐ·Ð°Ð½Ñ‹ Ñ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð°Ð¼Ð¸).
    ð»
    H (k Ã— n) â€” Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° "Ñ‚ÐµÐ¼" (Ð¾Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚, ÐºÐ°Ðº Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹ ÑÐ²ÑÐ·Ð°Ð½Ñ‹ Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸).
    Ð§Ñ‚Ð¾ Ð¾Ð½Ð¸ Ð¾Ð·Ð½Ð°Ñ‡Ð°ÑŽÑ‚?
    W (Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð¸ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð¾Ð²)

    Ð¡Ñ‚Ñ€Ð¾ÐºÐ¸ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ Ð¾Ð±ÑŠÐµÐºÑ‚Ð°Ð¼ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼).
    Ð¡Ñ‚Ð¾Ð»Ð±Ñ†Ñ‹ â€” ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð¶Ð°Ð½Ñ€Ñ‹ Ñ„Ð¸Ð»ÑŒÐ¼Ð¾Ð²).
    ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ð½Ð°ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¾Ð±ÑŠÐµÐºÑ‚ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð¸Ñ‚ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñƒ.
    2) H (Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²)

    Ð¡Ñ‚Ñ€Ð¾ÐºÐ¸ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ð¼ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð°Ð¼.
    Ð¡Ñ‚Ð¾Ð»Ð±Ñ†Ñ‹ â€” Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¼ Ñ„Ð¸Ð»ÑŒÐ¼Ð°Ð¼).
    ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ð½Ð°ÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ñ€Ð¸Ð·Ð½Ð°Ðº Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÑÑ Ðº Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñƒ.

    from sklearn.decomposition import NMF
    nmf = NMF(n_components=2)
    W = nmf.fit_transform(X)
    H = nmf.components_
    """
    return a + b

def nine(a, b):
    """
    1. Text Features (Ð¢ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸)
    Ð¢ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ â€” ÑÑ‚Ð¾ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð² Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸. 
    ÐŸÐ¾ÑÐºÐ¾Ð»ÑŒÐºÑƒ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹ ML Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ñ Ñ‡Ð¸ÑÐ»Ð°Ð¼Ð¸, Ñ‚ÐµÐºÑÑ‚ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð² Ñ‡Ð¸ÑÐ»Ð¾Ð²Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚.
    2. Bag of Words (ÐœÐµÑˆÐ¾Ðº ÑÐ»Ð¾Ð²)
    Bag of Words (BoW) â€” ÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð± Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°, Ð³Ð´Ðµ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ðµ ÑÐ»Ð¾Ð²Ð° Ð² Ñ‚ÐµÐºÑÑ‚Ðµ.
    ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚?
    Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ÑÑ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð²ÑÐµÑ… ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ»Ð¾Ð² Ð² Ñ‚ÐµÐºÑÑ‚Ð°Ñ….
    ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°, Ð³Ð´Ðµ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ€Ð°Ð· ÑÐ»Ð¾Ð²Ð¾ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¢ÐµÐºÑÑ‚Ñ‹:
    "I love cats"
    "I hate cats"
    Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ: ['I', 'love', 'hate', 'cats']
    Ð’ÐµÐºÑ‚Ð¾Ñ€Ñ‹:
    "I love cats" â†’ [1, 1, 0, 1]
    "I hate cats" â†’ [1, 0, 1, 1]
    ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹:
    Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐ»Ð¾Ð².
    ÐÐµ ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÑƒ (Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð²).

    3. Word Embeddings (Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð²)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    Word Embeddings â€” ÑÑ‚Ð¾ ÑÐ¿Ð¾ÑÐ¾Ð± Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð² Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð² Ð¼Ð½Ð¾Ð³Ð¾Ð¼ÐµÑ€Ð½Ð¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ, Ð³Ð´Ðµ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð±Ð»Ð¸Ð·ÐºÐ¸Ðµ ÑÐ»Ð¾Ð²Ð° Ð½Ð°Ñ…Ð¾Ð´ÑÑ‚ÑÑ Ð±Ð»Ð¸Ð·ÐºÐ¾ Ð´Ñ€ÑƒÐ³ Ðº Ð´Ñ€ÑƒÐ³Ñƒ.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹:
    Word2Vec.ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚?
    ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¼ ÐºÐ¾Ñ€Ð¿ÑƒÑÐµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð².
    ÐšÐ°Ð¶Ð´Ð¾Ðµ ÑÐ»Ð¾Ð²Ð¾ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 100-Ð¼ÐµÑ€Ð½Ð¾Ð³Ð¾).
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¡Ð»Ð¾Ð²Ð° "king" Ð¸ "queen" Ð±ÑƒÐ´ÑƒÑ‚ Ð±Ð»Ð¸Ð·ÐºÐ¸ Ð² Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ.
    Ð¡Ð»Ð¾Ð²Ð° "king" Ð¸ "apple" Ð±ÑƒÐ´ÑƒÑ‚ Ð´Ð°Ð»ÐµÐºÐ¸.
    4. Text Preprocessing (ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð°)
    ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð° â€” ÑÑ‚Ð¾ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð° Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°. Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚:
    Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð².
    Ð¡Ñ‚ÐµÐ¼Ð¼Ð¸Ð½Ð³ Ð¸Ð»Ð¸ Ð»ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ.
    ÐŸÑ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ðº Ð½Ð¸Ð¶Ð½ÐµÐ¼Ñƒ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ñƒ.
    Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸ Ð¸ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð².
    5. Stopwords (Ð¡Ñ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²Ð°)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    Ð¡Ñ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²Ð° â€” ÑÑ‚Ð¾ ÑÐ»Ð¾Ð²Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ Ð½ÐµÑÑƒÑ‚ ÑÐ¼Ñ‹ÑÐ»Ð¾Ð²Ð¾Ð¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "Ð¸", "Ð²", "Ð½Ð°").
    Ð—Ð°Ñ‡ÐµÐ¼ ÑƒÐ´Ð°Ð»ÑÑ‚ÑŒ?
    Ð£Ð¼ÐµÐ½ÑŒÑˆÐ°ÑŽÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ….
    Ð£Ð»ÑƒÑ‡ÑˆÐ°ÑŽÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚: "Ð¯ Ð¸Ð´Ñƒ Ð² Ð¼Ð°Ð³Ð°Ð·Ð¸Ð½"
    ÐŸÐ¾ÑÐ»Ðµ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²: "Ð¸Ð´Ñƒ Ð¼Ð°Ð³Ð°Ð·Ð¸Ð½"
    6. Stemming (Ð¡Ñ‚ÐµÐ¼Ð¼Ð¸Ð½Ð³)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    Ð¡Ñ‚ÐµÐ¼Ð¼Ð¸Ð½Ð³ â€” ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð²Ð° Ðº ÐµÐ³Ð¾ ÐºÐ¾Ñ€Ð½ÐµÐ²Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ðµ.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¡Ð»Ð¾Ð²Ð° "running", "runner", "runs" â†’ "run"
    7. Lemmatization (Ð›ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    Ð›ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ â€” ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð²Ð° Ðº ÐµÐ³Ð¾ ÑÐ»Ð¾Ð²Ð°Ñ€Ð½Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ðµ (Ð»ÐµÐ¼Ð¼Ðµ).
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¡Ð»Ð¾Ð²Ð° "running", "runner", "runs" â†’ "run"
    8. TF-IDF (Term Frequency-Inverse Document Frequency)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    TF-IDF â€” ÑÑ‚Ð¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¼ÐµÑ€Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÑÐ»Ð¾Ð²Ð° Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².
    ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚?
    TF (Term Frequency): Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð° ÑÐ»Ð¾Ð²Ð° Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ.
    IDF (Inverse Document Frequency): Ð›Ð¾Ð³Ð°Ñ€Ð¸Ñ„Ð¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°, ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‰ÐµÐ³Ð¾ ÑÐ»Ð¾Ð²Ð¾.
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¡Ð»Ð¾Ð²Ð¾ "the" Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ñ‡Ð°ÑÑ‚Ð¾, Ð½Ð¾ Ð½Ðµ Ð²Ð°Ð¶Ð½Ð¾ (Ð½Ð¸Ð·ÐºÐ¸Ð¹ TF-IDF).
    Ð¡Ð»Ð¾Ð²Ð¾ "cat" Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ñ€ÐµÐ´ÐºÐ¾, Ð½Ð¾ Ð²Ð°Ð¶Ð½Ð¾ (Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¹ TF-IDF).
    Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð°:
    TF-IDF(t,d)=TF(t,d)Ã—log( DF(t)N )
    Ð³Ð´Ðµ:
    t â€” Ñ‚ÐµÑ€Ð¼Ð¸Ð½ (ÑÐ»Ð¾Ð²Ð¾).
    d â€” Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚.
    N â€” Ð¾Ð±Ñ‰ÐµÐµ Ñ‡Ð¸ÑÐ»Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².
    DF(t) â€” Ñ‡Ð¸ÑÐ»Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð², ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‰Ð¸Ñ… Ñ‚ÐµÑ€Ð¼Ð¸Ð½ t.
    9. n-Grams (n-Ð³Ñ€Ð°Ð¼Ð¼Ñ‹)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    n-Grams â€” ÑÑ‚Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¸Ð· n ÑÐ»Ð¾Ð² Ð¸Ð»Ð¸ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð².
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð‘Ð¸Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ (2-Ð³Ñ€Ð°Ð¼Ð¼Ñ‹): "I love", "love cats"
    Ð¢Ñ€Ð¸Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ (3-Ð³Ñ€Ð°Ð¼Ð¼Ñ‹): "I love cats"
    Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐ»Ð¾Ð².
    10. LDA (Latent Dirichlet Allocation)
    Ð§Ñ‚Ð¾ ÑÑ‚Ð¾?
    LDA â€” ÑÑ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´ Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ…. 
    ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ ÐºÐ°Ðº ÑÐ¼ÐµÑÑŒ Ñ‚ÐµÐ¼.
    ÐšÐ°Ð¶Ð´Ð°Ñ Ñ‚ÐµÐ¼Ð° â€” ÑÑ‚Ð¾ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð².
    ÐŸÑ€Ð¸Ð¼ÐµÑ€:
    Ð¢ÐµÐ¼Ð° 1: "ÐºÐ¾ÑˆÐºÐ¸", "ÑÐ¾Ð±Ð°ÐºÐ¸", "Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ñ‹Ðµ"
    Ð¢ÐµÐ¼Ð° 2: "Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ", "Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹", "ÐºÐ¾Ð´"
    ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ:
    ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².
    ÐŸÐ¾Ð¸ÑÐº ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‚ÐµÐ¼.
    Description: LDA is a generative probabilistic model used for topic modeling. It assumes that
    documents are mixtures of topics and topics are mixtures of words.
    Steps:
    1. Initialize topic distributions for each document.
    2. Initialize word distributions for each topic.
    3. Iteratively update the distributions using Gibbs sampling or variational inference.
    """
    return a + b

